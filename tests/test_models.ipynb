{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88877bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\Abdessamad\\Desktop\\MLOpsClassificationTexteV2\\src\\models\")\n",
    "from base_model import BaseModel\n",
    "\n",
    "# from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "# from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "# import torch\n",
    "# import tens\n",
    "# from .base_model import BaseModel  # Assure-toi que l'importation est correcte\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd30a0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\Abdessamad\\Desktop\\MLOpsClassificationTexteV2\\src\\models\")\n",
    "\n",
    "class RandomForestModel(BaseModel):\n",
    "    _instance = None  # Variable pour stocker l'instance unique\n",
    "    _params = {}  # Dictionnaire pour stocker les paramètres\n",
    "\n",
    "    def __new__(cls, **params):\n",
    "        # Vérifie si l'instance existe déjà\n",
    "        if not cls._instance:\n",
    "            cls._instance = super(RandomForestModel, cls).__new__(cls)\n",
    "            # Enregistrer les paramètres si c'est la première instantiation\n",
    "            cls._params = params\n",
    "            # Initialisation du modèle avec les paramètres passés\n",
    "            cls._instance.model = RandomForestClassifier(\n",
    "                n_estimators=params.get(\"n_estimators\", 100),\n",
    "                max_depth=params.get(\"max_depth\", None),\n",
    "                random_state=params.get(\"random_state\", 42)\n",
    "            )\n",
    "        return cls._instance\n",
    "\n",
    "    def entrainer(self, X, y):\n",
    "        # Entraînement du modèle\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def evaluer(self, X, y):\n",
    "        # Évaluation du modèle (classification report)\n",
    "        y_pred = self.predire(X)\n",
    "        return classification_report(y, y_pred)\n",
    "\n",
    "    def predire(self, X):\n",
    "        # Prédictions du modèle\n",
    "        return self.model.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d291e096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Données jouets\n",
    "# # X = [\"I love this product\", \"This is awful\", \"Amazing experience\", \"Very bad\", \"Quite good\"]\n",
    "# # y = [1, 0, 1, 0, 1]\n",
    "\n",
    "# # Instancier le modèle\n",
    "# modele_bert = ModeleBERT(num_labels=2)\n",
    "\n",
    "# # Entraîner le modèle\n",
    "# modele_bert.entrainer(X, y)\n",
    "\n",
    "# # Évaluer le modèle\n",
    "# acc = modele_bert.evaluer(X, y)\n",
    "\n",
    "# # Afficher les résultats\n",
    "# print(\"Accuracy:\", acc)\n",
    "# print(\"Predictions:\", modele_bert.predire(X))\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# # from sklearn.metrics import accuracy_score\n",
    "# sys.path.append(r\"C:\\Users\\Abdessamad\\Desktop\\MLOpsClassificationTexteV2\\src\\data\")\n",
    "# import loader \n",
    "# Données jouets\n",
    "# X = [\"I love this product\", \"This is awful\", \"Amazing experience\", \"Very bad\", \"Quite good\"]\n",
    "# y = [1, 0, 1, 0, 1]\n",
    "# for chunk in loader.generer_donnees_test():\n",
    "#     X=chunk[\"text\"]\n",
    "#     y=chunk[\"label\"] # Affiche les 5 premières lignes du premier chunk\n",
    "#     break \n",
    "# #                   REGRISSION\n",
    "# # # Vectorisation TF-IDF\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# X_vect = vectorizer.fit_transform(X)\n",
    "# # Création du modèle Gradient Boosting\n",
    "# model = GradientBoostingModel(learning_rate=0.1, n_estimators=100, max_depth=3)\n",
    "# # Entraîner le modèle\n",
    "# model.entrainer(X_vect, y)\n",
    "\n",
    "# # Évaluer le modèle\n",
    "# acc = model.evaluer(X_vect, y)\n",
    "# print(acc)\n",
    "\n",
    "\n",
    "# # Prédire\n",
    "# predictions = model.predire(X_vect)\n",
    "# print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5940de2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Créer et entraîner le modèle\u001b[39;00m\n\u001b[32m     18\u001b[39m model = GradientBoostingModel(learning_rate=\u001b[32m0.1\u001b[39m, n_estimators=\u001b[32m20\u001b[39m, max_depth=\u001b[32m3\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mentrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_vect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Évaluer le modèle\u001b[39;00m\n\u001b[32m     22\u001b[39m acc = model.evaluer(X_vect, y)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mGradientBoostingModel.entrainer\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mentrainer\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[32m     26\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[33;03m    Entraîne le modèle avec les données X et y.\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Abdessamad\\Desktop\\MLOpsClassificationTexteV2\\venv\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Abdessamad\\Desktop\\MLOpsClassificationTexteV2\\venv\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:787\u001b[39m, in \u001b[36mBaseGradientBoosting.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, monitor)\u001b[39m\n\u001b[32m    784\u001b[39m     \u001b[38;5;28mself\u001b[39m._resize_state()\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m n_stages = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_stages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbegin_at_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[32m    801\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_stages != \u001b[38;5;28mself\u001b[39m.estimators_.shape[\u001b[32m0\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Abdessamad\\Desktop\\MLOpsClassificationTexteV2\\venv\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:883\u001b[39m, in \u001b[36mBaseGradientBoosting._fit_stages\u001b[39m\u001b[34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[39m\n\u001b[32m    876\u001b[39m         initial_loss = factor * \u001b[38;5;28mself\u001b[39m._loss(\n\u001b[32m    877\u001b[39m             y_true=y_oob_masked,\n\u001b[32m    878\u001b[39m             raw_prediction=raw_predictions[~sample_mask],\n\u001b[32m    879\u001b[39m             sample_weight=sample_weight_oob_masked,\n\u001b[32m    880\u001b[39m         )\n\u001b[32m    882\u001b[39m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m883\u001b[39m raw_predictions = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_stage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_csc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_csc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_csr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_csr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[38;5;66;03m# track loss\u001b[39;00m\n\u001b[32m    896\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Abdessamad\\Desktop\\MLOpsClassificationTexteV2\\venv\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:489\u001b[39m, in \u001b[36mBaseGradientBoosting._fit_stage\u001b[39m\u001b[34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[39m\n\u001b[32m    486\u001b[39m     sample_weight = sample_weight * sample_mask.astype(np.float64)\n\u001b[32m    488\u001b[39m X = X_csc \u001b[38;5;28;01mif\u001b[39;00m X_csc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m \u001b[43mtree\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_g_view\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m    491\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[38;5;66;03m# update tree leaves\u001b[39;00m\n\u001b[32m    494\u001b[39m X_for_tree_update = X_csr \u001b[38;5;28;01mif\u001b[39;00m X_csr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Abdessamad\\Desktop\\MLOpsClassificationTexteV2\\venv\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Abdessamad\\Desktop\\MLOpsClassificationTexteV2\\venv\\Lib\\site-packages\\sklearn\\tree\\_classes.py:1404\u001b[39m, in \u001b[36mDecisionTreeRegressor.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, check_input)\u001b[39m\n\u001b[32m   1374\u001b[39m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   1375\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight=\u001b[38;5;28;01mNone\u001b[39;00m, check_input=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m   1376\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[32m   1377\u001b[39m \n\u001b[32m   1378\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1401\u001b[39m \u001b[33;03m        Fitted estimator.\u001b[39;00m\n\u001b[32m   1402\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1404\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1406\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1407\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1408\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1409\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1410\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Abdessamad\\Desktop\\MLOpsClassificationTexteV2\\venv\\Lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[39m, in \u001b[36mBaseDecisionTree._fit\u001b[39m\u001b[34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    462\u001b[39m     builder = BestFirstTreeBuilder(\n\u001b[32m    463\u001b[39m         splitter,\n\u001b[32m    464\u001b[39m         min_samples_split,\n\u001b[32m   (...)\u001b[39m\u001b[32m    469\u001b[39m         \u001b[38;5;28mself\u001b[39m.min_impurity_decrease,\n\u001b[32m    470\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m \u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.n_outputs_ == \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    475\u001b[39m     \u001b[38;5;28mself\u001b[39m.n_classes_ = \u001b[38;5;28mself\u001b[39m.n_classes_[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics import accuracy_score\n",
    "sys.path.append(r\"C:\\Users\\Abdessamad\\Desktop\\MLOpsClassificationTexteV2\\src\\data\")\n",
    "import loader  # Assure-toi que loader est correctement importé\n",
    "\n",
    "# Charger les données\n",
    "# for chunk in loader.generer_donnees_test():\n",
    "#     X = chunk[\"text\"]  # Texte des données\n",
    "#     y = chunk[\"label\"]  # Labels des données\n",
    "#     break  # Prendre uniquement le premier chunk\n",
    "\n",
    "# # Vectorisation TF-IDF\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# X_vect = vectorizer.fit_transform(X)\n",
    "\n",
    "# # Créer et entraîner le modèle\n",
    "# # model = GradientBoostingModel(learning_rate=0.1, n_estimators=20, max_depth=3)\n",
    "# model.entrainer(X_vect, y)\n",
    "\n",
    "# # Évaluer le modèle\n",
    "# acc = model.evaluer(X_vect, y)\n",
    "# print(\"Classification Report:\\n\", acc)  # Afficher le rapport de classification\n",
    "\n",
    "# # Prédire sur de nouvelles données\n",
    "# predictions = model.predire(X_vect)\n",
    "# print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b729ef40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.86      0.92       189\n",
      "           1       1.00      0.81      0.89       204\n",
      "           2       0.72      0.97      0.83       245\n",
      "           3       0.79      0.95      0.86       215\n",
      "           4       1.00      0.57      0.73       147\n",
      "\n",
      "    accuracy                           0.85      1000\n",
      "   macro avg       0.90      0.83      0.85      1000\n",
      "weighted avg       0.89      0.85      0.85      1000\n",
      "\n",
      "Predictions: [0 0 0 0 0 2 1 3 3 2 1 3 0 2 3 3 3 0 0 2 1 2 1 2 2 2 1 3 3 1 3 3 0 4 0 0 4\n",
      " 3 0 1 2 1 3 1 4 0 2 1 3 1 3 2 2 0 3 3 1 1 3 2 4 1 0 3 3 4 0 3 1 2 3 1 4 3\n",
      " 2 2 0 2 1 2 0 1 1 3 1 0 2 2 0 4 2 3 1 3 3 1 2 3 0 3 0 3 3 1 1 0 3 3 1 2 1\n",
      " 2 1 4 1 0 3 1 2 2 2 2 2 2 2 2 2 4 2 3 2 3 4 4 0 2 2 3 1 0 2 0 2 4 2 2 1 0\n",
      " 0 2 1 1 3 3 1 3 1 0 2 3 2 2 2 1 3 2 4 2 2 4 2 3 1 1 1 0 1 0 4 3 3 4 2 1 1\n",
      " 0 0 0 4 1 1 2 2 3 1 0 1 0 0 3 4 4 1 3 4 2 3 3 2 0 1 2 1 3 3 2 3 2 3 3 4 2\n",
      " 1 4 3 3 2 1 3 2 2 3 0 0 2 3 2 2 2 2 4 1 4 0 4 3 0 3 1 3 2 2 3 0 3 1 2 2 1\n",
      " 2 3 2 2 1 2 2 2 2 1 3 0 1 0 2 4 2 2 2 4 4 2 2 4 3 3 0 4 0 2 1 2 3 1 4 1 3\n",
      " 2 0 4 3 4 3 2 3 3 3 1 2 3 1 3 3 1 2 1 4 0 2 0 2 4 1 1 2 2 1 2 4 3 1 4 2 4\n",
      " 2 2 0 2 0 0 2 2 0 2 2 2 0 3 3 2 3 3 2 2 1 3 0 2 0 2 2 2 2 1 3 1 3 0 2 4 2\n",
      " 3 3 3 3 1 1 1 1 0 3 2 3 2 3 1 0 0 3 1 2 2 0 1 2 1 0 0 3 4 2 4 1 3 4 3 3 3\n",
      " 3 1 2 2 2 2 3 1 3 0 0 1 2 1 3 1 3 3 1 2 2 0 2 3 0 1 0 0 0 2 2 2 3 2 2 2 3\n",
      " 2 1 3 4 3 3 2 1 3 2 1 2 2 2 4 0 3 1 2 3 3 1 2 2 1 2 4 2 3 1 2 3 4 2 2 3 0\n",
      " 2 2 2 3 3 1 2 3 1 3 2 0 3 1 3 2 2 2 3 3 1 2 3 3 3 0 2 2 2 0 0 0 2 0 4 3 1\n",
      " 2 1 3 3 2 0 3 3 3 2 3 2 0 3 4 4 2 0 2 3 4 4 0 3 3 3 3 3 3 3 3 4 4 1 3 0 1\n",
      " 2 3 4 2 0 3 3 4 0 4 3 2 3 2 1 3 2 1 2 2 2 3 2 2 2 2 2 2 1 2 3 1 2 0 2 3 2\n",
      " 2 0 3 3 2 3 2 2 3 3 2 2 3 1 2 2 3 2 3 2 4 2 3 1 2 2 0 3 2 2 2 2 0 1 3 4 1\n",
      " 1 1 2 2 1 3 4 1 2 2 0 3 2 3 2 3 2 2 0 3 2 1 4 0 3 0 0 1 2 3 2 3 4 3 2 1 0\n",
      " 1 0 2 2 3 1 2 1 3 3 2 3 2 0 0 0 3 0 3 2 3 3 3 1 1 2 2 2 3 0 0 2 0 3 2 4 3\n",
      " 2 2 2 2 1 1 3 3 2 2 1 2 1 2 1 1 2 4 0 0 3 3 1 2 2 2 0 2 3 4 3 2 3 0 2 2 1\n",
      " 0 0 1 1 3 0 2 2 3 4 2 3 0 0 1 0 2 0 3 3 0 0 2 2 1 2 2 2 3 1 2 2 3 3 2 2 0\n",
      " 2 3 1 0 2 3 3 2 3 2 3 3 2 0 2 0 2 1 0 0 3 3 0 2 2 2 3 1 1 0 4 3 3 3 0 3 2\n",
      " 0 3 1 2 4 1 0 0 1 0 3 0 0 1 2 0 2 3 2 0 4 3 3 2 2 0 3 2 1 2 4 2 2 3 3 3 3\n",
      " 3 4 2 2 4 4 2 2 3 2 0 0 3 3 2 0 0 0 2 1 2 2 1 3 3 2 3 2 1 0 3 4 3 2 2 0 1\n",
      " 2 0 3 1 2 1 2 2 3 2 2 2 4 4 4 2 4 0 2 1 2 1 0 0 2 0 2 1 2 1 4 2 4 2 2 2 0\n",
      " 0 0 0 0 2 2 2 0 2 2 3 2 3 2 0 4 3 3 3 3 3 1 0 0 2 1 2 0 3 1 2 2 3 2 2 2 3\n",
      " 4 3 2 3 0 2 1 3 2 2 0 0 4 0 2 1 3 1 4 3 0 1 2 3 0 2 3 1 3 3 2 4 0 1 2 1 2\n",
      " 3]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "sys.path.append(r\"C:\\Users\\Abdessamad\\Desktop\\MLOpsClassificationTexteV2\\src\\data\")\n",
    "import loader  # Assure-toi que loader est correctement importé\n",
    "\n",
    "# Charger les données\n",
    "for chunk in loader.generer_donnees_test():\n",
    "    X = chunk[\"text\"]  # Texte des données\n",
    "    y = chunk[\"label\"]  # Labels des données\n",
    "    break  # Prendre uniquement le premier chunk\n",
    "\n",
    "# Vectorisation TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_vect = vectorizer.fit_transform(X)\n",
    "\n",
    "# Créer et entraîner le modèle Random Forest\n",
    "model = RandomForestModel(n_estimators=100, max_depth=10, random_state=42)\n",
    "model.entrainer(X_vect, y)\n",
    "\n",
    "# Évaluer le modèle\n",
    "acc = model.evaluer(X_vect, y)\n",
    "print(\"Classification Report:\\n\", acc)\n",
    "\n",
    "# Prédire sur de nouvelles données\n",
    "predictions = model.predire(X_vect)\n",
    "print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccf018fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Abdessamad\\Desktop\\MLOpsClassificationTexteV2\\venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\Abdessamad\\Desktop\\MLOpsClassificationTexteV2\\src\\models\")\n",
    "from base_model import BaseModel\n",
    "class MLPModel(BaseModel):\n",
    "\n",
    "    def __init__(self, hidden_layer_sizes=(100,), activation='relu', solver='adam', max_iter=200):\n",
    "        self.model = MLPClassifier(\n",
    "            hidden_layer_sizes=hidden_layer_sizes,\n",
    "            activation=activation,\n",
    "            solver=solver,\n",
    "            max_iter=max_iter\n",
    "        )\n",
    "\n",
    "    def entrainer(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def evaluer(self, X, y):\n",
    "        y_pred = self.predire(X)\n",
    "        return classification_report(y, y_pred)\n",
    "\n",
    "    def predire(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Charger les données\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Séparer en ensemble d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardiser les données (important pour MLP)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Tester le modèle\n",
    "model = MLPModel(hidden_layer_sizes=(50,), max_iter=300)\n",
    "model.entrainer(X_train, y_train)\n",
    "rapport = model.evaluer(X_test, y_test)\n",
    "\n",
    "print(rapport)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
