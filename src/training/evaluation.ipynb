{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5405e5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(r\"C:\\Users\\Abdessamad\\Desktop\\MLOpsClassificationTexteV2\\src\")\n",
    "import data.loader as loader\n",
    "import training.train as train\n",
    "import pandas as pd\n",
    "import random\n",
    "from models.factory import get_model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import warnings\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from typing import List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca20f957",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000/\")\n",
    "mlflow.set_experiment(\"MLOpsClassificationTexte\")\n",
    "mlruns_dir = r\"C:/Users/Abdessamad/Desktop/MLOpsClassificationTexteV2/mlruns\"\n",
    "custom_artifacts_dir = r\"C:/Users/Abdessamad/Desktop/MLOpsClassificationTexteV2/mlartifacts\"\n",
    "os.makedirs(mlruns_dir, exist_ok=True)\n",
    "os.makedirs(custom_artifacts_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c5f669",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test: List[str] = []\n",
    "y_test: np.ndarray = np.array([])\n",
    "# max_chunks = 3\n",
    "# count = 0\n",
    "for chunk in loader.generer_donnees_test():\n",
    "    textes:List[str] = chunk[\"text\"].astype(str).tolist()\n",
    "    X_test.extend(textes)  # Liste plate\n",
    "    y_test = np.append(y_test, np.array(chunk[\"label\"]))\n",
    "    # count += 1\n",
    "    # # y_test = np.where(y_test <=2, 0, 1)\n",
    "    # if count >= max_chunks:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb22dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLflow_1()->None:\n",
    "    max_iters:List[int] = [random.randint(10, 100) for _ in range(1)]\n",
    "    for max_iter in max_iters:\n",
    "        #                               logistic\n",
    "        with mlflow.start_run(run_name=\"logistic\"):        \n",
    "            mlflow.log_param(\"model\", \"logistic\")\n",
    "            mlflow.log_param(\"max_iter\",max_iter)\n",
    "            rapport_train,model,vectorizer=train.Model(\"logistic\",max_iter)\n",
    "            X_vect_test = vectorizer.transform(X_test)\n",
    "            # print(rapport_train)\n",
    "            mlflow.log_metric('accuracy_train', rapport_train['accuracy'])\n",
    "            mlflow.log_metric('recall_class_1_train', rapport_train['1']['recall'])\n",
    "            mlflow.log_metric('recall_class_0_train', rapport_train['0']['recall'])\n",
    "            mlflow.log_metric('f1_score_macro_train', rapport_train['macro avg']['f1-score'])\n",
    "            rapport_test=model.evaluer(X_vect_test,y_test)\n",
    "            # print(rapport_test)\n",
    "            mlflow.log_metric('accuracy_test', rapport_test['accuracy'])\n",
    "            mlflow.log_metric('recall_class_1_test', rapport_test['1.0']['recall'])\n",
    "            mlflow.log_metric('recall_class_0_test', rapport_test['0.0']['recall'])\n",
    "            mlflow.log_metric('f1_score_macro_test', rapport_test['macro avg']['f1-score'])\n",
    "        print(\"logistic est termines\")\n",
    "        #                               MLPClassifier\n",
    "        with mlflow.start_run(run_name=\"MLPClassifier\"):        \n",
    "            mlflow.log_param(\"model\", \"MLPClassifier\")\n",
    "            mlflow.log_param(\"max_iter\",max_iter)\n",
    "            rapport_train,model,vectorizer=train.Model(\"MLPClassifier\",max_iter=max_iter)\n",
    "            mlflow.log_metric('accuracy_train', rapport_train['accuracy'])\n",
    "            mlflow.log_metric('recall_class_1_train', rapport_train['1']['recall'])\n",
    "            mlflow.log_metric('recall_class_0_train', rapport_train['0']['recall'])\n",
    "            mlflow.log_metric('f1_score_macro_train', rapport_train['macro avg']['f1-score'])\n",
    "            rapport_test=model.evaluer(X_vect_test,y_test)\n",
    "            mlflow.log_metric('accuracy_test', rapport_test['accuracy'])\n",
    "            mlflow.log_metric('recall_class_1_test', rapport_test['1.0']['recall'])\n",
    "            mlflow.log_metric('recall_class_0_test', rapport_test['0.0']['recall'])\n",
    "            mlflow.log_metric('f1_score_macro_test', rapport_test['macro avg']['f1-score'])\n",
    "        print(\"MLPClassifier est termines\")\n",
    "\n",
    "# MLflow_1()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73e62b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlflow_2()->None:\n",
    "    n_estimators: List[int]=[random.randint(50, 100) for _ in range(2)]\n",
    "    learning_rates: List[float]=[0.1,0.2]\n",
    "    max_depths: List[int]=[random.randint(1, 20) for _ in range(1)]\n",
    "    # 'relu', 'logistic'\n",
    "    for n in n_estimators:\n",
    "        for max_depth in max_depths:\n",
    "            with mlflow.start_run(run_name=\"RandomForestModel\"):        \n",
    "                mlflow.log_param(\"model\", \"RandomForestModel\")\n",
    "                mlflow.log_param(\"n_estimators\",n)\n",
    "                mlflow.log_param(\"max_depth\",max_depth)\n",
    "                rapport_train,model,vectorizer=train.Model(\"RandomForestModel\",n_estimators=n,max_depth=max_depth)\n",
    "                X_vect_test = vectorizer.transform(X_test)\n",
    "                mlflow.log_metric('accuracy_train', rapport_train['accuracy'])\n",
    "                mlflow.log_metric('recall_class_1_train', rapport_train['1']['recall'])\n",
    "                mlflow.log_metric('recall_class_0_train', rapport_train['0']['recall'])\n",
    "                mlflow.log_metric('f1_score_macro_train', rapport_train['macro avg']['f1-score'])\n",
    "                rapport_test=model.evaluer(X_vect_test,y_test)\n",
    "                mlflow.log_metric('accuracy_test', rapport_test['accuracy'])\n",
    "                mlflow.log_metric('recall_class_1_test', rapport_test['1.0']['recall'])\n",
    "                mlflow.log_metric('recall_class_0_test', rapport_test['0.0']['recall'])\n",
    "                mlflow.log_metric('f1_score_macro_test', rapport_test['macro avg']['f1-score'])\n",
    "            print(\"RandomForestModel est termines\") \n",
    "            for learning_rate in learning_rates:\n",
    "                with mlflow.start_run(run_name=\"GradientBoostingModel\"):        \n",
    "                    mlflow.log_param(\"model\", \"GradientBoostingModel\")\n",
    "                    mlflow.log_param(\"n_estimators\",n)\n",
    "                    mlflow.log_param(\"max_depth\",max_depth)\n",
    "                    mlflow.log_param(\"learning_rate\",learning_rate)\n",
    "                    rapport_train,model,vectorizer=train.Model(\"GradientBoostingModel\",n_estimators=n,learning_rate=learning_rate,max_depth=max_depth)\n",
    "                    mlflow.log_metric('accuracy_train', rapport_train['accuracy'])\n",
    "                    mlflow.log_metric('recall_class_1_train', rapport_train['1']['recall'])\n",
    "                    mlflow.log_metric('recall_class_0_train', rapport_train['0']['recall'])\n",
    "                    mlflow.log_metric('f1_score_macro_train', rapport_train['macro avg']['f1-score'])\n",
    "                    rapport_test=model.evaluer(X_vect_test,y_test)\n",
    "                    mlflow.log_metric('accuracy_test', rapport_test['accuracy'])\n",
    "                    mlflow.log_metric('recall_class_1_test', rapport_test['1.0']['recall'])\n",
    "                    mlflow.log_metric('recall_class_0_test', rapport_test['0.0']['recall'])\n",
    "                    mlflow.log_metric('f1_score_macro_test', rapport_test['macro avg']['f1-score'])\n",
    "                print(\"GradientBoostingModel est termines\") \n",
    "\n",
    "mlflow_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec7148f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with mlflow.start_run(run_name=\"MLPClassifier\"):        \n",
    "#     mlflow.log_param(\"model\", \"MLPClassifier\")\n",
    "#     mlflow.log_param(\"max_iter\",100)\n",
    "#     print(\"1\")\n",
    "#     rapport_train,model,vectorizer=train.Model(\"MLPClassifier\",max_iter=100)\n",
    "#     X_vect_test = vectorizer.transform(X_test)\n",
    "#     print(\"1\")\n",
    "#     mlflow.log_metric('accuracy_train', rapport_train['accuracy'])\n",
    "#     mlflow.log_metric('recall_class_1_train', rapport_train['1']['recall'])\n",
    "#     mlflow.log_metric('recall_class_0_train', rapport_train['0']['recall'])\n",
    "#     mlflow.log_metric('f1_score_macro_train', rapport_train['macro avg']['f1-score'])\n",
    "#     print(\"1\")\n",
    "#     rapport_test=model.evaluer(X_vect_test,y_test)\n",
    "#     print(\"1\")\n",
    "#     mlflow.log_metric('accuracy_test', rapport_test['accuracy'])\n",
    "#     mlflow.log_metric('recall_class_1_test', rapport_test['1']['recall'])\n",
    "#     mlflow.log_metric('recall_class_0_test', rapport_test['0']['recall'])\n",
    "#     mlflow.log_metric('f1_score_macro_test', rapport_test['macro avg']['f1-score'])\n",
    "# print(\"MLPClassifier est termines\")  \n",
    "# y_test=y_test.astype(int)\n",
    "# @log_execution_time\n",
    "# @log_function_call\n",
    "# n_estimator=100\n",
    "# learning_rate=0.1\n",
    "# max_depth=6\n",
    "# # 'relu', 'logistic'\n",
    "# with mlflow.start_run(run_name=\"RandomForestModel\"):        \n",
    "#     mlflow.log_param(\"model\", \"RandomForestModel\")\n",
    "#     mlflow.log_param(\"n_estimators\",n_estimator)\n",
    "#     mlflow.log_param(\"max_depth\",max_depth)\n",
    "#     print(\"1\")\n",
    "#     rapport_train,model,vectorizer=train.Model(\"RandomForestModel\",n_estimators=n_estimator,max_depth=max_depth)\n",
    "#     X_vect_test = vectorizer.transform(X_test)\n",
    "#     print(rapport_train)\n",
    "#     print(rapport_train.keys())\n",
    "#     mlflow.log_metric('accuracy_train', rapport_train['accuracy'])\n",
    "#     mlflow.log_metric('recall_class_1_train', rapport_train['1']['recall'])\n",
    "#     mlflow.log_metric('recall_class_0_train', rapport_train['0']['recall'])\n",
    "#     mlflow.log_metric('f1_score_macro_train', rapport_train['macro avg']['f1-score'])\n",
    "#     print(\"1\")\n",
    "#     rapport_test=model.evaluer(X_vect_test,y_test)\n",
    "#     print(rapport_test)\n",
    "#     mlflow.log_metric('accuracy_test', rapport_test['accuracy'])\n",
    "#     mlflow.log_metric('recall_class_1_test', rapport_test['1']['recall'])\n",
    "#     mlflow.log_metric('recall_class_0_test', rapport_test['0']['recall'])\n",
    "#     mlflow.log_metric('f1_score_macro_test', rapport_test['macro avg']['f1-score'])\n",
    "# print(\"RandomForestModel est termines\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db800184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with mlflow.start_run(run_name=\"MLPClassifier\"):        \n",
    "#     mlflow.log_param(\"model\", \"MLPClassifier\")\n",
    "#     mlflow.log_param(\"max_iter\",100)\n",
    "#     print(\"1\")\n",
    "#     rapport_train,model,vectorizer=train.Model(\"MLPClassifier\",max_iter=100)\n",
    "#     X_vect_test = vectorizer.transform(X_test)\n",
    "#     print(\"1\")\n",
    "#     mlflow.log_metric('accuracy_train', rapport_train['accuracy'])\n",
    "#     mlflow.log_metric('recall_class_1_train', rapport_train['1']['recall'])\n",
    "#     mlflow.log_metric('recall_class_0_train', rapport_train['0']['recall'])\n",
    "#     mlflow.log_metric('f1_score_macro_train', rapport_train['macro avg']['f1-score'])\n",
    "#     print(\"1\")\n",
    "#     rapport_test=model.evaluer(X_vect_test,y_test)\n",
    "#     print(\"1\")\n",
    "#     mlflow.log_metric('accuracy_test', rapport_test['accuracy'])\n",
    "#     mlflow.log_metric('recall_class_1_test', rapport_test['1']['recall'])\n",
    "#     mlflow.log_metric('recall_class_0_test', rapport_test['0']['recall'])\n",
    "#     mlflow.log_metric('f1_score_macro_test', rapport_test['macro avg']['f1-score'])\n",
    "# print(\"MLPClassifier est termines\")  \n",
    "# y_test=y_test.astype(int)\n",
    "# @log_execution_time\n",
    "# @log_function_call\n",
    "# n_estimator=100\n",
    "# learning_rate=0.1\n",
    "# max_depth=6\n",
    "# # 'relu', 'logistic'\n",
    "# with mlflow.start_run(run_name=\"RandomForestModel\"):        \n",
    "#     mlflow.log_param(\"model\", \"RandomForestModel\")\n",
    "#     mlflow.log_param(\"n_estimators\",n_estimator)\n",
    "#     mlflow.log_param(\"max_depth\",max_depth)\n",
    "#     print(\"1\")\n",
    "#     rapport_train,model,vectorizer=train.Model(\"RandomForestModel\",n_estimators=n_estimator,max_depth=max_depth)\n",
    "#     X_vect_test = vectorizer.transform(X_test)\n",
    "#     print(\"1\")\n",
    "#     mlflow.log_metric('accuracy_train', rapport_train['accuracy'])\n",
    "#     mlflow.log_metric('recall_class_1_train', rapport_train['1']['recall'])\n",
    "#     mlflow.log_metric('recall_class_0_train', rapport_train['0']['recall'])\n",
    "#     mlflow.log_metric('f1_score_macro_train', rapport_train['macro avg']['f1-score'])\n",
    "#     print(\"1\")\n",
    "#     rapport_test=model.evaluer(X_vect_test,y_test)\n",
    "#     print(\"1\")\n",
    "#     mlflow.log_metric('accuracy_test', rapport_test['accuracy'])\n",
    "#     mlflow.log_metric('recall_class_1_test', rapport_test['1']['recall'])\n",
    "#     mlflow.log_metric('recall_class_0_test', rapport_test['0']['recall'])\n",
    "#     mlflow.log_metric('f1_score_macro_test', rapport_test['macro avg']['f1-score'])\n",
    "# print(\"RandomForestModel est termines\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6dcc361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with mlflow.start_run(run_name=\"GradientBoostingModel\"):        \n",
    "#     mlflow.log_param(\"model\", \"GradientBoostingModel\")\n",
    "#     mlflow.log_param(\"n_estimators\",n_estimator)\n",
    "#     mlflow.log_param(\"max_depth\",max_depth)\n",
    "#     mlflow.log_param(\"learning_rate\",learning_rate)\n",
    "#     rapport_train,model,vectorizer=train.Model(\"GradientBoostingModel\",n_estimators=n_estimator,learning_rate=learning_rate,max_depth=max_depth)\n",
    "#     mlflow.log_metric('accuracy_train', rapport_train['accuracy'])\n",
    "#     mlflow.log_metric('recall_class_1_train', rapport_train['1']['recall'])\n",
    "#     mlflow.log_metric('recall_class_0_train', rapport_train['0']['recall'])\n",
    "#     mlflow.log_metric('f1_score_macro_train', rapport_train['macro avg']['f1-score'])\n",
    "#     rapport_test=model.evaluer(X_vect_test,y_test)\n",
    "#     mlflow.log_metric('accuracy_test', rapport_test['accuracy'])\n",
    "#     mlflow.log_metric('recall_class_1_test', rapport_test['1.0']['recall'])\n",
    "#     mlflow.log_metric('recall_class_0_test', rapport_test['0.0']['recall'])\n",
    "#     mlflow.log_metric('f1_score_macro_test', rapport_test['macro avg']['f1-score'])\n",
    "# print(\"GradientBoostingModel est termines\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d94a38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Positive', 'Negative', 'Positive', 'Positive', 'Positive']\n"
     ]
    }
   ],
   "source": [
    "commentaires = [\n",
    "    \"Epic and emotional story.\",\n",
    "    \"Terrible acting and bad plot.\",\n",
    "    \"A masterpiece of modern cinema.\",\n",
    "    \"Boring and too long.\",\n",
    "    \"Amazing visual effects!\"\n",
    "]\n",
    "def a():\n",
    "    # Chargement du modèle, du vectorizer, etc.\n",
    "    _, model, vectorizer = train.Model(\"logistic\", 25)\n",
    "\n",
    "    # Transformation du texte avec le vectorizer\n",
    "    vect_data = vectorizer.transform(commentaires)\n",
    "\n",
    "    # Prédiction\n",
    "    predictions = model.predire(vect_data)\n",
    "\n",
    "    results = [\"Positive\" if p == 1 else \"Negative\" for p in predictions]\n",
    "    return results\n",
    "\n",
    "print(a())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
